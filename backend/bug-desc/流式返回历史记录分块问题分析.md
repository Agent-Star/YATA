# 流式返回历史记录分块问题分析

## 问题描述

**现象**：虽然流式返回的实时渲染效果很好，但刷新前端页面返回历史记录后，之前 NLU 返回的内容被分成了许多小块（每个块只有 1-2 个字符）。

**影响**：用户体验差，历史记录不可读。

## 问题定位

通过对 Backend 和 Frontend 的深入分析，确定问题发生在 **Backend 侧**。

### 根本原因

**LangGraph 的消息保存机制导致每个 AIMessageChunk 都被当作独立消息保存到 checkpoint**。

### 详细分析

#### 1. feat/backend 的流式实现

在 `backend/src/agents/travel_planner.py` 中：

```python
# Line 101-109: 接收 NLU 的 token 事件，创建 AIMessageChunk
elif event_type == "token":
    delta = event.get("delta", "")
    full_content += delta

    chunk = AIMessageChunk(content=delta)
    chunk = cast(AIMessageChunk, add_timestamp_to_message(chunk))
    chunks.append(chunk)

# Line 150-154: 返回所有 chunks
return {
    "messages": chunks,  # ❌ 问题所在：返回了多个 chunks
    "nlu_response": nlu_response,
    "fallback_triggered": False,
}
```

#### 2. LangGraph 的 MessagesState 行为

`MessagesState` 使用 `add_messages` reducer：

```python
class TravelPlannerState(MessagesState, total=False):
    ...
```

**关键特性**：

- `add_messages` reducer 会将节点返回的所有消息添加到状态的 `messages` 列表中
- 在流式模式下，如果返回 `{"messages": [chunk1, chunk2, ..., chunkN]}`，这 N 个 chunks **都会被保存**到 checkpoint
- LangGraph **不会自动合并**这些 chunks

#### 3. 流式输出 vs. 状态保存

在 `planner_routes.py` 的流式端点中：

```python
# Line 231-249: 流式输出
async for stream_event in agent.astream(user_input, config=config, stream_mode=["messages"], subgraphs=True):
    ...
    if stream_mode == "messages":
        msg, _ = event
        if isinstance(msg, AIMessageChunk):
            # 流式输出每个 chunk ✅ 这是正确的
            yield f"data: {json.dumps({'type': 'token', 'delta': ...})}\n\n"
```

**流式输出正常工作**，用户能实时看到生成过程。

#### 4. 历史记录读取

在 `planner_routes.py` 的历史记录端点中：

```python
# Line 146-162: 从 checkpoint 读取所有消息
state = await agent.aget_state(config=config)
messages: list[AnyMessage] = state.values.get("messages", [])

# Line 162: 转换为前端格式
frontend_messages = [langchain_message_to_frontend(msg) for msg in filtered_messages]
```

**问题**：

- 从 checkpoint 读取的 `messages` 包含所有保存的 AIMessageChunk
- 每个 chunk 都被转换为一个独立的 `FrontendMessage`
- 前端渲染时，每个 chunk 显示为一个独立的消息块

#### 5. Frontend 渲染

Frontend 没有问题，它只是按照 Backend 返回的数据进行渲染。每个 `FrontendMessage` 被渲染为一个消息框。

## 问题根源总结

```
┌─────────────────────────────────────────────────────────────┐
│ 问题链路                                                    │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  travel_planner.call_nlu_service()                          │
│    └─> 返回 {"messages": [chunk1, chunk2, ..., chunkN]}    │
│                                                             │
│  ↓                                                          │
│                                                             │
│  LangGraph MessagesState (add_messages reducer)            │
│    └─> 将所有 N 个 chunks 添加到状态中                     │
│    └─> 保存到 checkpoint                                   │
│                                                             │
│  ↓                                                          │
│                                                             │
│  agent.astream(stream_mode=["messages"])                   │
│    └─> 流式输出每个 chunk ✅ 实时渲染正常                  │
│                                                             │
│  ↓                                                          │
│                                                             │
│  agent.aget_state()                                        │
│    └─> 从 checkpoint 读取所有 N 个 chunks                  │
│    └─> 每个 chunk 被转换为独立的 FrontendMessage          │
│                                                             │
│  ↓                                                          │
│                                                             │
│  Frontend 渲染                                              │
│    └─> 显示 N 个独立的消息框 ❌ 历史记录被分块             │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

## 解决方案

### 方案 A: 只保存完整的 AIMessage（推荐）

**核心思路**：区分"流式输出的值"和"保存到状态的值"。

**实现方式**：修改 `travel_planner.py`，返回单个完整的 `AIMessage` 而不是多个 `AIMessageChunk`。

```python
# 在 travel_planner.py 的 call_nlu_service 函数中

# 收集完所有 chunks 后，只创建一个完整的 AIMessage
final_message = AIMessage(content=full_content)
final_message = cast(AIMessage, add_timestamp_to_message(final_message))

return {
    "messages": [final_message],  # ✅ 只返回一个完整消息
    "nlu_response": nlu_response,
    "fallback_triggered": False,
}
```

**优点**：

- ✅ 最简单、最直接的解决方案
- ✅ 历史记录显示为单条完整消息
- ✅ 不需要修改其他代码

**缺点**：

- ❌ **流式输出会丢失** - 因为我们只返回最终的完整消息，LangGraph 无法逐 token 流式输出

**适用场景**：如果可以接受牺牲流式输出，这是最简单的方案。

---

### 方案 B: 使用 LangGraph Functional API

**核心思路**：使用 `@entrypoint()` 装饰器，它支持 `entrypoint.final(value=..., save=...)` 来区分流式输出和保存的值。

**实现方式**：将 `travel_planner` 从 StateGraph 改为 Functional API。

```python
from langgraph.func import entrypoint

@entrypoint()
async def call_nlu_service(
    inputs: dict,
    *,
    previous: dict,
    config: RunnableConfig,
):
    # ... 接收 NLU 流式事件，收集 chunks ...

    # 创建完整的消息用于保存
    final_message = AIMessage(content=full_content)
    final_message = add_timestamp_to_message(final_message)

    # 区分流式输出和保存的值
    return entrypoint.final(
        value={"messages": chunks},  # 流式输出：返回所有 chunks
        save={"messages": [final_message]}  # 保存到 checkpoint：只保存完整消息
    )
```

**优点**：

- ✅ 完美解决问题 - 流式输出和历史记录都正确
- ✅ 符合 LangGraph 的设计模式

**缺点**：

- ❌ 需要重构 travel_planner 从 StateGraph 到 Functional API
- ❌ 改动较大，需要测试
- ⚠️  Functional API 在 LangGraph v1.0 后可能有变化

**适用场景**：长期方案，适合完全重构。

---

### 方案 C: 后处理 - 合并历史记录中的 chunks（临时方案）

**核心思路**：在读取历史记录时，检测并合并连续的 `AIMessageChunk`。

**实现方式**：修改 `planner_routes.py` 的 `get_history` 端点，添加消息合并逻辑。

```python
# 在 planner_routes.py 的 get_history 函数中

# Line 151 之后，添加合并逻辑
messages: list[AnyMessage] = state.values.get("messages", [])

# 合并连续的 AIMessageChunk
merged_messages = merge_consecutive_chunks(messages)

# 后续继续使用 merged_messages
filtered_messages = (
    [msg for msg in merged_messages if not isinstance(msg, ToolMessage)]
    if filter_tool_message
    else merged_messages
)

# 辅助函数
def merge_consecutive_chunks(messages: list[AnyMessage]) -> list[AnyMessage]:
    """合并连续的 AIMessageChunk 为单个 AIMessage"""
    from langchain_core.messages import AIMessage, AIMessageChunk

    merged = []
    current_chunks = []

    for msg in messages:
        if isinstance(msg, AIMessageChunk) and type(msg) == AIMessageChunk:
            # 收集连续的 chunks
            current_chunks.append(msg)
        else:
            # 遇到非 chunk 消息，先合并之前的 chunks
            if current_chunks:
                merged_content = "".join(str(chunk.content) for chunk in current_chunks)
                # 使用第一个 chunk 的时间戳
                first_chunk = current_chunks[0]
                merged_msg = AIMessage(
                    content=merged_content,
                    additional_kwargs=getattr(first_chunk, "additional_kwargs", {}),
                    response_metadata=getattr(first_chunk, "response_metadata", {}),
                )
                # 复制 id
                if hasattr(first_chunk, "id"):
                    merged_msg.id = first_chunk.id
                merged.append(merged_msg)
                current_chunks = []

            # 添加当前消息
            merged.append(msg)

    # 处理最后的 chunks
    if current_chunks:
        merged_content = "".join(str(chunk.content) for chunk in current_chunks)
        first_chunk = current_chunks[0]
        merged_msg = AIMessage(
            content=merged_content,
            additional_kwargs=getattr(first_chunk, "additional_kwargs", {}),
            response_metadata=getattr(first_chunk, "response_metadata", {}),
        )
        if hasattr(first_chunk, "id"):
            merged_msg.id = first_chunk.id
        merged.append(merged_msg)

    return merged
```

**优点**：

- ✅ 不影响流式输出 - 实时渲染仍然正常工作
- ✅ 修改范围小 - 只需修改历史记录读取逻辑
- ✅ 快速修复 - 无需重构核心逻辑

**缺点**：

- ⚠️  治标不治本 - checkpoint 中仍然保存着大量 chunks
- ⚠️  增加历史记录读取的开销（需要遍历并合并）
- ⚠️  可能影响其他依赖消息列表的功能

**适用场景**：快速修复，短期方案。

---

## 推荐方案

### 短期（立即修复）：方案 C - 后处理合并

- 快速解决用户痛点
- 不影响现有流式功能
- 风险低，改动小

### 长期（彻底解决）：方案 A + 改进流式机制

1. 修改 travel_planner 只保存完整的 AIMessage
2. 研究是否可以在节点内部使用特殊机制来支持流式输出（例如使用 LangGraph 的 streaming writer API）

## 相关代码位置

| 文件 | 行号 | 说明 |
|------|------|------|
| `backend/src/agents/travel_planner.py` | 101-109 | 创建 AIMessageChunk |
| `backend/src/agents/travel_planner.py` | 150-154 | 返回 chunks 列表 ❌ |
| `backend/src/service/planner_routes.py` | 146-162 | 历史记录读取 |
| `backend/src/service/planner_routes.py` | 231-249 | 流式输出 ✅ |
| `backend/src/service/planner_routes.py` | 77-121 | 消息转换函数 |

## 测试验证

修复后需要验证：

1. ✅ 实时流式渲染正常（逐 token 显示）
2. ✅ 刷新页面后历史记录显示为单条完整消息
3. ✅ 消息时间戳正确
4. ✅ 消息收藏功能正常

## 总结

- **问题根源**：Backend 的 travel_planner 返回了多个 AIMessageChunk，LangGraph 将它们全部保存到 checkpoint
- **责任定位**：Backend 侧的实现问题，Frontend 和 NLU 实现都正常
- **推荐方案**：短期使用方案 C 快速修复，长期考虑方案 A 或 B 彻底解决
