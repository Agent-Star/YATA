# IMP-002: Agent å¤šè¯­è¨€æ”¯æŒä¼˜åŒ–

## å…ƒæ•°æ®

- **ID**: IMP-002
- **åˆ†ç±»**: å›½é™…åŒ–
- **ä¼˜å…ˆçº§**: ğŸŸ¢ ä½
- **çŠ¶æ€**: å¾…å¤„ç†
- **åˆ›å»ºæ—¥æœŸ**: 2025-01-27
- **é¢„è®¡å·¥ä½œé‡**: ä¸­
- **ç›¸å…³æ–‡æ¡£**: `compliance-check.md`, `system-flow-analysis.md`

---

## é—®é¢˜æè¿°

### å½“å‰å®ç°

#### 1. æ¥å£å±‚å·²æ”¯æŒè¯­è¨€é…ç½®

**æ–‡ä»¶**: `backend/src/service/planner_routes.py`

```python
@planner_router.post("/plan/stream")
async def plan_stream(request: PlanRequest, ...):
    # æ„å»ºé…ç½®
    configurable: dict[str, Any] = {
        "thread_id": thread_id,
        "user_id": str(current_user.id),
    }
    
    # âœ… æ¥æ”¶å¹¶ä¼ é€’è¯­è¨€é…ç½®
    if request.context.language:
        configurable["language"] = request.context.language
```

#### 2. Agent å±‚æœªä½¿ç”¨è¯­è¨€é…ç½®

**æ–‡ä»¶**: `backend/src/agents/research_assistant.py`

```python
async def acall_model(state: AgentState, config: RunnableConfig) -> AgentState:
    m = get_model(config["configurable"].get("model", settings.DEFAULT_MODEL))
    model_runnable = wrap_model(m)
    response = await model_runnable.ainvoke(state, config)
    # ...
```

**é—®é¢˜**ï¼š`wrap_model()` å‡½æ•°ç”Ÿæˆç³»ç»Ÿæç¤ºè¯æ—¶ï¼Œæ²¡æœ‰è€ƒè™‘ `config["configurable"]["language"]`ã€‚

### ä¸è¶³ä¹‹å¤„

1. **è¯­è¨€ä¸ä¸€è‡´**ï¼šå³ä½¿å‰ç«¯ä¼ é€’äº† `language="zh"`ï¼ŒAgent ä»å¯èƒ½ç”¨è‹±æ–‡å›å¤
2. **ç”¨æˆ·ä½“éªŒå·®**ï¼šç”¨æˆ·æœŸæœ›çš„è¯­è¨€ä¸å®é™…å“åº”ä¸åŒ¹é…
3. **é…ç½®æœªç”Ÿæ•ˆ**ï¼šæ¥å£æ”¯æŒäº†è¯­è¨€é…ç½®ï¼Œä½†æ²¡æœ‰å®é™…ä½œç”¨

**ç¤ºä¾‹åœºæ™¯**ï¼š

```json
// å‰ç«¯è¯·æ±‚
{
  "prompt": "å¸®æˆ‘è§„åˆ’ä¸œäº¬æ—…è¡Œ",
  "context": { "language": "zh" }
}

// Agent å“åº”ï¼ˆå¯èƒ½æ˜¯è‹±æ–‡ï¼‰
"Sure! Here's a 3-day Tokyo itinerary..."
// âŒ ç”¨æˆ·æœŸæœ›ä¸­æ–‡å›å¤
```

---

## å½±å“åˆ†æ

### åŠŸèƒ½å½±å“

- âš ï¸ **è¯­è¨€ä¸ç¬¦åˆé¢„æœŸ**ï¼šå½±å“ç”¨æˆ·ä½“éªŒï¼Œä½†ä¸å½±å“æ ¸å¿ƒåŠŸèƒ½
- âœ… **Prompt å¯ä»¥å¼•å¯¼**ï¼šç”¨æˆ·ç”¨ä¸­æ–‡æé—®æ—¶ï¼ŒLLM é€šå¸¸ä¼šç”¨ä¸­æ–‡å›å¤ï¼ˆéƒ¨åˆ†åœºæ™¯å¯ä»¥work aroundï¼‰

### æ€§èƒ½å½±å“

- âœ… æ— æ˜¾è‘—æ€§èƒ½å½±å“

### ç”¨æˆ·ä½“éªŒå½±å“

- âš ï¸ **å›½é™…åŒ–ä½“éªŒå·®**ï¼šéè‹±è¯­ç”¨æˆ·å¯èƒ½æ”¶åˆ°è‹±æ–‡å›å¤
- âš ï¸ **é…ç½®ä¸ç”Ÿæ•ˆ**ï¼šç”¨æˆ·è®¾ç½®äº†è¯­è¨€åå¥½ä½†æ— æ•ˆ

### å¼€å‘ç»´æŠ¤å½±å“

- âœ… æ”¹è¿›åæ›´å®¹æ˜“æ”¯æŒå¤šè¯­è¨€
- âœ… ç¬¦åˆæœ€ä½³å®è·µ

---

## æ”¹è¿›æ–¹æ¡ˆ

### æ–¹æ¡ˆ 1: ç³»ç»Ÿæç¤ºè¯å›½é™…åŒ–ï¼ˆæ¨èï¼‰

**ä¼˜åŠ¿**ï¼š

- âœ… ç®€å•ç›´æ¥ï¼Œæ”¹åŠ¨æœ€å°
- âœ… LLM ç†è§£æ˜ç¡®çš„è¯­è¨€æŒ‡ä»¤
- âœ… æ”¯æŒæ‰€æœ‰ä¸»æµè¯­è¨€

**å®æ–½æ­¥éª¤**ï¼š

#### 1. å®šä¹‰è¯­è¨€æŒ‡ä»¤æ˜ å°„

**æ–‡ä»¶**: `backend/src/agents/research_assistant.py`

```python
from datetime import datetime

# è¯­è¨€æŒ‡ä»¤æ˜ å°„
LANGUAGE_INSTRUCTIONS = {
    "zh": "è¯·ç”¨ä¸­æ–‡ï¼ˆç®€ä½“ï¼‰å›ç­”æ‰€æœ‰é—®é¢˜ã€‚",
    "zh-TW": "è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”æ‰€æœ‰å•é¡Œã€‚",
    "en": "Please respond in English.",
    "ja": "æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚",
    "ko": "í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”ã€‚",
    "fr": "Veuillez rÃ©pondre en franÃ§ais.",
    "es": "Por favor, responde en espaÃ±ol.",
    "de": "Bitte antworten Sie auf Deutsch.",
    "it": "Si prega di rispondere in italiano.",
    "pt": "Por favor, responda em portuguÃªs.",
}

current_date = datetime.now().strftime("%B %d, %Y")

def get_instructions(language: str | None = None) -> str:
    """è·å–ç³»ç»Ÿæç¤ºè¯ï¼ŒåŒ…å«è¯­è¨€æŒ‡ä»¤"""
    base_instructions = f"""
    You are a helpful research assistant with the ability to search the web and use other tools.
    Today's date is {current_date}.

    NOTE: THE USER CAN'T SEE THE TOOL RESPONSE.

    A few things to remember:
    - Please include markdown-formatted links to any citations used in your response. Only include one
    or two citations per response unless more are needed. ONLY USE LINKS RETURNED BY THE TOOLS.
    - Use calculator tool with numexpr to answer math questions. The user does not understand numexpr,
      so for the final response, use human readable format - e.g. "300 * 200", not "(300 \\times 200)".
    """
    
    # æ·»åŠ è¯­è¨€æŒ‡ä»¤
    language_instruction = ""
    if language and language in LANGUAGE_INSTRUCTIONS:
        language_instruction = f"\n\nIMPORTANT: {LANGUAGE_INSTRUCTIONS[language]}"
    
    return base_instructions + language_instruction
```

#### 2. åœ¨ wrap_model ä¸­ä½¿ç”¨è¯­è¨€é…ç½®

```python
def wrap_model(model: BaseChatModel) -> RunnableSerializable[AgentState, AIMessage]:
    """
    æ³¨æ„ï¼šè¿™é‡Œè¿”å›çš„ Runnable ä¼šåœ¨è°ƒç”¨æ—¶æ¥æ”¶ state å’Œ config
    éœ€è¦åœ¨ preprocessor ä¸­æå– language
    """
    bound_model = model.bind_tools(tools)
    
    # åˆ›å»ºåŠ¨æ€çš„ preprocessor
    def create_messages_with_language(state_and_config):
        """ä» state å’Œ config åˆ›å»ºæ¶ˆæ¯åˆ—è¡¨"""
        # LangChain ä¼šä¼ é€’ (state, config) æˆ–åªä¼ é€’ state
        if isinstance(state_and_config, tuple):
            state, config = state_and_config
        else:
            state = state_and_config
            config = {}
        
        # æå–è¯­è¨€é…ç½®
        language = config.get("configurable", {}).get("language")
        instructions = get_instructions(language)
        
        return [SystemMessage(content=instructions)] + state.get("messages", [])
    
    preprocessor = RunnableLambda(
        create_messages_with_language,
        name="StateModifier",
    )
    
    return preprocessor | bound_model  # type: ignore[return-value]
```

**æ³¨æ„**ï¼šLangGraph çš„ Runnable è°ƒç”¨æœºåˆ¶æ¯”è¾ƒå¤æ‚ï¼Œéœ€è¦æµ‹è¯•ç¡®ä¿ config æ­£ç¡®ä¼ é€’ã€‚

#### 3. ç®€åŒ–æ–¹æ¡ˆï¼šåœ¨ acall_model ä¸­å¤„ç†

å¦‚æœä¸Šè¿°æ–¹æ¡ˆè¿‡äºå¤æ‚ï¼Œå¯ä»¥åœ¨ `acall_model` ä¸­ç›´æ¥å¤„ç†ï¼š

```python
async def acall_model(state: AgentState, config: RunnableConfig) -> AgentState:
    # æå–è¯­è¨€é…ç½®
    language = config.get("configurable", {}).get("language")
    
    # è·å–æ¨¡å‹
    m = get_model(config["configurable"].get("model", settings.DEFAULT_MODEL))
    bound_model = m.bind_tools(tools)
    
    # æ„å»ºå¸¦è¯­è¨€æŒ‡ä»¤çš„æ¶ˆæ¯
    instructions = get_instructions(language)
    messages = [SystemMessage(content=instructions)] + state["messages"]
    
    # è°ƒç”¨æ¨¡å‹
    response = await bound_model.ainvoke(messages, config)
    
    # ... åç»­å¤„ç† ...
    
    return {"messages": [response]}
```

---

### æ–¹æ¡ˆ 2: ä½¿ç”¨ Few-shot ç¤ºä¾‹

**ä¼˜åŠ¿**ï¼š

- âœ… æ›´ç¨³å®šçš„è¯­è¨€è¾“å‡º
- âœ… å¯ä»¥æ§åˆ¶è¾“å‡ºé£æ ¼

**åŠ£åŠ¿**ï¼š

- âŒ éœ€è¦å‡†å¤‡å¤šè¯­è¨€ç¤ºä¾‹
- âŒ å¢åŠ  token æ¶ˆè€—

**ç¤ºä¾‹**ï¼š

```python
def get_instructions_with_examples(language: str | None = None) -> str:
    base = get_instructions(language)
    
    if language == "zh":
        examples = """
        
        Example conversation:
        User: å¸®æˆ‘æŸ¥ä¸€ä¸‹ä»Šå¤©çš„å¤©æ°”
        Assistant: å¥½çš„ï¼Œæˆ‘æ¥ä¸ºæ‚¨æŸ¥è¯¢ä»Šå¤©çš„å¤©æ°”æƒ…å†µã€‚[ä½¿ç”¨å¤©æ°”å·¥å…·]
        æ ¹æ®æŸ¥è¯¢ç»“æœï¼Œä»Šå¤©åŒ—äº¬å¤©æ°”æ™´æœ—ï¼Œæ¸©åº¦15-25åº¦ï¼Œé€‚åˆå¤–å‡ºæ´»åŠ¨ã€‚
        """
        return base + examples
    
    return base
```

---

### æ–¹æ¡ˆ 3: Prompt Template ç³»ç»Ÿ

**ä¼˜åŠ¿**ï¼š

- âœ… æ›´çµæ´»ï¼Œæ”¯æŒå¤æ‚çš„å›½é™…åŒ–éœ€æ±‚
- âœ… å¯ä»¥å¤ç”¨æ¨¡æ¿

**åŠ£åŠ¿**ï¼š

- âŒ éœ€è¦é¢å¤–çš„æ¨¡æ¿ç®¡ç†ç³»ç»Ÿ
- âŒ å·¥ä½œé‡è¾ƒå¤§

**ä¸æ¨è**ï¼Œå½“å‰éœ€æ±‚ä¸éœ€è¦å¦‚æ­¤å¤æ‚çš„ç³»ç»Ÿã€‚

---

## å®æ–½å»ºè®®

### æ¨èæ–¹æ¡ˆ

**æ–¹æ¡ˆ 1 ç®€åŒ–ç‰ˆï¼ˆåœ¨ acall_model ä¸­å¤„ç†ï¼‰** - æœ€ä½³å¹³è¡¡

**ç†ç”±**ï¼š

1. å®ç°ç®€å•ï¼Œå®¹æ˜“ç†è§£
2. ä¸æ¶‰åŠå¤æ‚çš„ Runnable ç»„åˆ
3. æ»¡è¶³å½“å‰éœ€æ±‚

### å®æ–½æ­¥éª¤

1. **æ·»åŠ è¯­è¨€æŒ‡ä»¤æ˜ å°„**
   - åœ¨ `research_assistant.py` ä¸­å®šä¹‰ `LANGUAGE_INSTRUCTIONS`
   - å®ç° `get_instructions(language)` å‡½æ•°
   - é¢„è®¡å·¥ä½œé‡ï¼š30 åˆ†é’Ÿ

2. **ä¿®æ”¹ acall_model å‡½æ•°**
   - æå– `language` é…ç½®
   - ä½¿ç”¨ `get_instructions(language)` æ„å»ºç³»ç»Ÿæ¶ˆæ¯
   - é¢„è®¡å·¥ä½œé‡ï¼š30 åˆ†é’Ÿ

3. **å…¶ä»– Agent åŒæ­¥**ï¼ˆå¯é€‰ï¼‰
   - å¯¹ `chatbot`, `rag_assistant` ç­‰åº”ç”¨ç›¸åŒæ”¹åŠ¨
   - é¢„è®¡å·¥ä½œé‡ï¼š1 å°æ—¶

4. **æµ‹è¯•éªŒè¯**
   - æµ‹è¯•ä¸åŒè¯­è¨€çš„å“åº”
   - é¢„è®¡å·¥ä½œé‡ï¼š1 å°æ—¶

**æ€»è®¡**ï¼šçº¦ 3 å°æ—¶ï¼ˆåŒ…å«å…¶ä»– Agentï¼‰

### æ³¨æ„äº‹é¡¹

1. **è¯­è¨€ä»£ç æ ‡å‡†**ï¼šä½¿ç”¨ ISO 639-1 ä»£ç ï¼ˆå¦‚ `zh`, `en`, `ja`ï¼‰
2. **å›é€€æœºåˆ¶**ï¼šå¦‚æœä¸æ”¯æŒçš„è¯­è¨€ï¼Œä½¿ç”¨è‹±æ–‡
3. **ç”¨æˆ· Prompt ä¼˜å…ˆ**ï¼šå¦‚æœç”¨æˆ· prompt æœ¬èº«åŒ…å«è¯­è¨€æŒ‡ç¤ºï¼ˆå¦‚"ç”¨ä¸­æ–‡å›ç­”"ï¼‰ï¼Œåº”è¯¥ä¼˜å…ˆéµå¾ª
4. **LLM èƒ½åŠ›ä¾èµ–**ï¼šä¸æ˜¯æ‰€æœ‰ LLM éƒ½æ“…é•¿æ‰€æœ‰è¯­è¨€

### å›æ»šæ–¹æ¡ˆ

ç›´æ¥ç§»é™¤è¯­è¨€æŒ‡ä»¤éƒ¨åˆ†ï¼Œç³»ç»Ÿä¼šå›é€€åˆ°å½“å‰è¡Œä¸ºï¼ˆæ ¹æ®ç”¨æˆ· prompt è‡ªåŠ¨åˆ¤æ–­è¯­è¨€ï¼‰ã€‚

---

## æµ‹è¯•è®¡åˆ’

### å•å…ƒæµ‹è¯•

```python
def test_get_instructions_with_language():
    """æµ‹è¯•ä¸åŒè¯­è¨€çš„æŒ‡ä»¤ç”Ÿæˆ"""
    # ä¸­æ–‡
    zh_instructions = get_instructions("zh")
    assert "ä¸­æ–‡" in zh_instructions
    
    # è‹±æ–‡
    en_instructions = get_instructions("en")
    assert "English" in en_instructions
    
    # ä¸æ”¯æŒçš„è¯­è¨€ï¼ˆå›é€€åˆ°æ— è¯­è¨€æŒ‡ä»¤ï¼‰
    unknown_instructions = get_instructions("xyz")
    assert unknown_instructions == get_instructions(None)
```

### é›†æˆæµ‹è¯•

```bash
# æµ‹è¯•ä¸­æ–‡å“åº”
curl -X POST http://localhost:8080/planner/plan/stream \
  -H "Cookie: yata_auth=<token>" \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "What is the weather today?",
    "context": {"language": "zh"}
  }'

# æœŸæœ›ï¼šAgent ç”¨ä¸­æ–‡å›å¤ï¼ˆå³ä½¿ prompt æ˜¯è‹±æ–‡ï¼‰

# æµ‹è¯•æ—¥è¯­å“åº”
curl -X POST http://localhost:8080/planner/plan/stream \
  -H "Cookie: yata_auth=<token>" \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "ä»Šæ—¥ã®å¤©æ°—ã¯ï¼Ÿ",
    "context": {"language": "ja"}
  }'

# æœŸæœ›ï¼šAgent ç”¨æ—¥è¯­å›å¤
```

### äººå·¥æµ‹è¯•æ¸…å•

- [ ] å‘é€è‹±æ–‡ prompt + ä¸­æ–‡è¯­è¨€é…ç½® â†’ æ”¶åˆ°ä¸­æ–‡å›å¤
- [ ] å‘é€ä¸­æ–‡ prompt + è‹±æ–‡è¯­è¨€é…ç½® â†’ æ”¶åˆ°è‹±æ–‡å›å¤
- [ ] å‘é€ä¸­æ–‡ prompt + æ— è¯­è¨€é…ç½® â†’ æ”¶åˆ°ä¸­æ–‡å›å¤ï¼ˆLLM è‡ªåŠ¨åˆ¤æ–­ï¼‰
- [ ] æµ‹è¯•å¤šç§è¯­è¨€ï¼ˆè‡³å°‘ 5 ç§ï¼‰
- [ ] æµ‹è¯•ä¸æ”¯æŒçš„è¯­è¨€ä»£ç  â†’ ç³»ç»Ÿæ­£å¸¸å·¥ä½œï¼ˆå›é€€åˆ°æ— è¯­è¨€æŒ‡ä»¤ï¼‰

---

## å·²çŸ¥é™åˆ¶

### 1. LLM èƒ½åŠ›é™åˆ¶

ä¸æ˜¯æ‰€æœ‰ LLM éƒ½æ“…é•¿æ‰€æœ‰è¯­è¨€ï¼š

- GPT-4: å‡ ä¹æ‰€æœ‰ä¸»æµè¯­è¨€éƒ½å¾ˆå¥½
- Claude: ä¸»æµè¯­è¨€è‰¯å¥½
- æŸäº›å¼€æºæ¨¡å‹: å¯èƒ½ä»…æ”¯æŒè‹±æ–‡å’Œæœ‰é™çš„å…¶ä»–è¯­è¨€

**å»ºè®®**ï¼šåœ¨æ–‡æ¡£ä¸­è¯´æ˜æ”¯æŒçš„è¯­è¨€åˆ—è¡¨ã€‚

### 2. å·¥å…·å“åº”è¯­è¨€

å·¥å…·ï¼ˆå¦‚ Web Searchï¼‰è¿”å›çš„å†…å®¹å¯èƒ½æ˜¯å¤šç§è¯­è¨€ï¼ŒAgent éœ€è¦ç¿»è¯‘æˆ–æ€»ç»“ã€‚è¿™éƒ¨åˆ†ä¾èµ– LLM èƒ½åŠ›ã€‚

### 3. æ··åˆè¯­è¨€åœºæ™¯

å¦‚æœç”¨æˆ·åœ¨ä¸€ä¸ªä¼šè¯ä¸­åˆ‡æ¢è¯­è¨€ï¼ˆå¦‚å…ˆç”¨ä¸­æ–‡ï¼Œåç”¨è‹±æ–‡ï¼‰ï¼Œéœ€è¦å†³å®šæ˜¯ï¼š

- é€‰é¡¹ A: æ¯æ¬¡ä½¿ç”¨æœ€æ–°çš„è¯­è¨€é…ç½®
- é€‰é¡¹ B: ä¿æŒä¼šè¯åˆå§‹è¯­è¨€
- **å»ºè®®**ï¼šé€‰é¡¹ Aï¼ˆæ¯æ¬¡ä½¿ç”¨æœ€æ–°é…ç½®ï¼Œæ›´çµæ´»ï¼‰

---

## æ‰©å±•å»ºè®®

### 1. è¯­è¨€è‡ªåŠ¨æ£€æµ‹

å¦‚æœå‰ç«¯æ²¡æœ‰ä¼ é€’ `language` é…ç½®ï¼Œå¯ä»¥ä»ç”¨æˆ· prompt è‡ªåŠ¨æ£€æµ‹è¯­è¨€ï¼š

```python
from langdetect import detect

def detect_language(text: str) -> str | None:
    """è‡ªåŠ¨æ£€æµ‹æ–‡æœ¬è¯­è¨€"""
    try:
        lang = detect(text)
        return lang
    except:
        return None

# åœ¨ acall_model ä¸­ä½¿ç”¨
language = config.get("configurable", {}).get("language")
if not language:
    # å°è¯•ä»æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯æ£€æµ‹
    last_user_message = next((m for m in reversed(state["messages"]) if isinstance(m, HumanMessage)), None)
    if last_user_message:
        language = detect_language(last_user_message.content)
```

**æ³¨æ„**ï¼šéœ€è¦æ·»åŠ  `langdetect` ä¾èµ–ã€‚

### 2. åŒºåŸŸåŒ–é…ç½®

é™¤äº†è¯­è¨€ï¼Œè¿˜å¯ä»¥æ”¯æŒåŒºåŸŸé…ç½®ï¼ˆå¦‚æ—¥æœŸæ ¼å¼ã€è´§å¸æ ¼å¼ï¼‰ï¼š

```python
configurable = {
    "thread_id": thread_id,
    "user_id": str(current_user.id),
    "language": "zh",
    "locale": "zh-CN",  # åŒºåŸŸ
    "timezone": "Asia/Shanghai",  # æ—¶åŒº
}
```

### 3. å¤šè¯­è¨€çŸ¥è¯†åº“

å¦‚æœä½¿ç”¨ RAGï¼Œéœ€è¦ä¸ºä¸åŒè¯­è¨€å‡†å¤‡å¯¹åº”çš„çŸ¥è¯†åº“ï¼Œæˆ–ä½¿ç”¨å¤šè¯­è¨€ Embedding æ¨¡å‹ã€‚

---

## ç›¸å…³èµ„æº

- [ISO 639-1 Language Codes](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes)
- [OpenAI Multi-language Support](https://platform.openai.com/docs/guides/gpt-best-practices)
- [LangChain Internationalization](https://python.langchain.com/docs/modules/model_io/prompts/)

---

## æ›´æ–°æ—¥å¿—

- 2025-01-27: åˆ›å»ºæ–‡æ¡£ï¼Œæä¾›ç³»ç»Ÿæç¤ºè¯å›½é™…åŒ–æ–¹æ¡ˆ
