"""
ğŸŒ æ—…è¡Œè§„åˆ’ RAG æ•°æ®ä¾›ç»™ç³»ç»Ÿï¼ˆBGE-M3 ç‰ˆï¼‰
åŠŸèƒ½ï¼š
- OSMï¼šè·å–åŸå¸‚åœ°ç†åæ ‡
- Wikipediaï¼šå¤šè¯­è¨€æ–‡æœ¬ + Infoboxï¼ˆä¸å†ä¼˜å…ˆä¸­æ–‡ï¼‰
- WeatherAPIï¼šè·å–å†å²æ°”æ¸©
- BAAI/bge-m3ï¼šå‘é‡åŒ–ï¼ˆ1024ç»´ï¼‰
- è¾“å‡º JSON æ–‡ä»¶ â†’ å«æ¸…æ´—åçŸ¥è¯†ï¼Œä¾›ä¸‹æ¸¸ä½¿ç”¨

ğŸ“Œ ç‰¹ç‚¹ï¼šä½¿ç”¨ BGE-M3 æ¨¡å‹ï¼ˆ1024ç»´ï¼‰ã€æ”¯æŒä¸­è‹±åŒè¯­ã€å¸¦ Infobox æ¸…æ´—
"""

import json
import os
import re
from datetime import datetime, timedelta

import requests
from sentence_transformers import SentenceTransformer

# ===================== é…ç½® =====================
# è¾“å‡ºç›®å½•ï¼šå½“å‰é¡¹ç›®çš„ data ç›®å½•
OUTPUT_DIR = os.path.join(os.path.dirname(__file__), "data")

WIKI_LANGUAGES = ["en", "zh"]  # è‹±æ–‡ä¼˜å…ˆï¼Œä¸­æ–‡ fallback
USER_AGENT = "TravelRAG-Agent/1.0 (contact@team.com)"

# æ›¿æ¢ä¸ºä½ è‡ªå·±çš„ WeatherAPI Key
WEATHERAPI_KEY = "your-key-here"

# ä½¿ç”¨ BGE-M3 æ¨¡å‹ï¼ˆ1024ç»´ï¼Œæ”¯æŒå¤šè¯­è¨€ï¼‰
MODEL_NAME = "BAAI/bge-m3"
model = SentenceTransformer(MODEL_NAME)


# ===================== å·¥å…·å‡½æ•° =====================
def clean_text(text):
    if not text:
        return ""
    return re.sub(r"\s+", " ", text).strip()


def split_text(text, min_len=200, max_len=500):
    cleaned = clean_text(text)
    if not cleaned:
        return []
    sentences = re.split(r"[ã€‚ï¼Ÿï¼.?!\n]", cleaned)
    chunks, current, length = [], [], 0
    for sent in sentences:
        sent = sent.strip()
        if not sent:
            continue
        l = len(sent)
        if length + l > max_len:
            if current:
                chunks.append("".join(current))
                current, length = [sent], l
            else:
                chunks.append(sent)
        elif length + l >= min_len:
            current.append(sent)
            chunks.append("".join(current))
            current, length = [], 0
        else:
            current.append(sent)
            length += l
    if current:
        chunks.append("".join(current))
    return [c for c in chunks if len(c) >= 100]


def vectorize_chunks(chunks):
    if not chunks:
        return []
    embeddings = model.encode(chunks, normalize_embeddings=True)
    return list(zip(chunks, embeddings.tolist()))


# ===================== 1. OSM åœ°ç†æ•°æ® =====================
def get_osm_data(city_name, country_code):
    url = "https://nominatim.openstreetmap.org/search"
    params = {
        "q": city_name,
        "format": "json",
        "limit": 1,
        "countrycodes": country_code,
        "class": "boundary",
        "type": "administrative",
    }
    headers = {"User-Agent": USER_AGENT}

    try:
        response = requests.get(url, params=params, headers=headers, timeout=10)
        response.raise_for_status()
        data = response.json()
        if not data:
            print(f"âŒ OSMæœªæ‰¾åˆ°ï¼š{city_name}({country_code})")
            return None
        item = data[0]
        return {
            "lat": float(item["lat"]),
            "lon": float(item["lon"]),
            "country": item["display_name"].split(",")[-1].strip(),
        }
    except Exception as e:
        print(f"âŒ OSMå¤±è´¥ï¼š{e}")
        return None


# ===================== 2. Wikipedia å¤šè¯­è¨€çŸ¥è¯† =====================
def clean_infobox(raw_infobox):
    """
    ç®€åŒ–ç‰ˆ Infobox æ¸…æ´—ï¼šæå–æ¸©åº¦ã€é™æ°´ã€åˆ«åç­‰å…³é”®å­—æ®µ
    """
    if not raw_infobox:
        return {}

    cleaned = {}

    # æ¸©åº¦æ˜ å°„ï¼ˆå…¼å®¹å¤§å°å†™å’Œç¬¦å·å˜ä½“ï¼‰
    temp_map = {
        "Jan_Hi_Â°C": "jan_high_temp",
        "Feb_Hi_Â°C": "feb_high_temp",
        "Mar_Hi_Â°C": "mar_high_temp",
        "Apr_Hi_Â°C": "apr_high_temp",
        "May_Hi_Â°C": "may_high_temp",
        "Jun_Hi_Â°C": "jun_high_temp",
        "Jul_Hi_Â°C": "jul_high_temp",
        "Aug_Hi_Â°C": "aug_high_temp",
        "Sep_Hi_Â°C": "sep_high_temp",
        "Oct_Hi_Â°C": "oct_high_temp",
        "Nov_Hi_Â°C": "nov_high_temp",
        "Dec_Hi_Â°C": "dec_high_temp",
        "Jan_Lo_Â°C": "jan_low_temp",
        "Year_Precip_mm": "annual_precipitation_mm",
    }

    for key, clean_key in temp_map.items():
        if key in raw_infobox:
            try:
                val = raw_infobox[key].strip().replace("Â°C", "").replace(",", "")
                cleaned[clean_key] = round(float(val), 1)
            except:
                pass

    # æ–‡æœ¬å­—æ®µæå–
    text_fields = {
        "nickname": "nickname",
        "official_name": "official_name",
        "country": "country",
        "area_total_km2": "area_sqkm",
        "population_as_of": "population_year",
    }
    for key, desc in text_fields.items():
        if key in raw_infobox:
            val = re.sub(
                r"\[\[.*?\]\]",
                lambda m: m.group(0).split("|")[-1].strip("]]"),
                raw_infobox[key],
            )
            val = re.sub(r"<.*?>", "", val).strip()
            if val:
                cleaned[desc] = val

    # æ¨æ–­æœ€ä½³æ—…æ¸¸å­£èŠ‚
    summer_avg = cleaned.get("jul_high_temp", 0)
    winter_avg = cleaned.get("jan_high_temp", 0)
    if summer_avg > 30 and winter_avg < 15:
        cleaned["best_travel_season"] = "Spring/Autumn"
    elif summer_avg > 25:
        cleaned["best_travel_season"] = "Autumn"
    elif winter_avg > 18:
        cleaned["best_travel_season"] = "Winter warmth"
    else:
        cleaned["best_travel_season"] = "Spring/Autumn"

    return cleaned


def get_wikipedia_data(city_name, country_code):
    """
    ä¼˜å…ˆå°è¯•è‹±æ–‡é¡µé¢ï¼Œå¤±è´¥å fallback åˆ°ä¸­æ–‡
    """
    for lang in WIKI_LANGUAGES:
        wiki_url = f"https://{lang}.wikipedia.org/w/api.php"
        headers = {"User-Agent": USER_AGENT}

        direct_titles = [city_name, f"{city_name} (city)", f"{city_name} City"]

        for title in direct_titles:
            try:
                content_params = {
                    "action": "query",
                    "prop": "extracts|revisions",
                    "titles": title,
                    "explaintext": True,
                    "rvprop": "content",
                    "rvslots": "main",
                    "format": "json",
                }
                res = requests.get(
                    wiki_url, params=content_params, headers=headers, timeout=10
                )
                res.raise_for_status()
                data = res.json()
                page = list(data["query"]["pages"].values())[0]

                if page.get("missing") or len(page.get("extract", "").strip()) < 50:
                    continue

                extract = page.get("extract", "")
                full_title = page["title"]
                page_url = f"https://{lang}.wikipedia.org/wiki/{full_title}"

                rev = page.get("revisions", [{}])[0]
                wikitext = None
                if "slots" in rev and "main" in rev["slots"]:
                    wikitext = rev["slots"]["main"].get("*")
                elif "*" in rev:
                    wikitext = rev["*"]
                if not wikitext:
                    continue

                import mwparserfromhell

                wikicode = mwparserfromhell.parse(wikitext)
                templates = wikicode.filter_templates()
                infobox_candidates = [
                    t
                    for t in templates
                    if "infobox" in str(t.name).lower() or "ä¿¡æ¯æ¡†" in str(t.name)
                ]

                raw_infobox = {}
                if infobox_candidates:
                    chosen = infobox_candidates[0]
                    for param in chosen.params:
                        key = str(param.name).strip()
                        value = str(param.value).strip()
                        value = re.sub(r"\[\[(?:[^|\]]*\|)?([^]]+)\]\]", r"\1", value)
                        if key and value:
                            raw_infobox[key] = value

                structured_knowledge = clean_infobox(raw_infobox)

                print(f"âœ… [{lang}] æˆåŠŸè·å–ï¼šã€Š{full_title}ã€‹")
                return extract, structured_knowledge, lang, full_title, page_url

            except Exception as e:
                print(f"âŒ [{lang}] è¯·æ±‚å¤±è´¥ {title}ï¼š{str(e)}")
                continue

    print(f"âŒ æ‰€æœ‰å°è¯•å‡å¤±è´¥ï¼š{city_name}")
    return None, None, None, None, None


# ===================== 3. å¤©æ°”æ•°æ® ======================
def get_weather_data(lat, lon):
    if not WEATHERAPI_KEY or "YOUR_" in WEATHERAPI_KEY:
        print("ğŸŸ¡ è·³è¿‡å¤©æ°”æ•°æ®")
        return None

    try:
        target_date = (datetime.now() - timedelta(days=2)).strftime("%Y-%m-%d")
        url = "http://api.weatherapi.com/v1/history.json"
        params = {"key": WEATHERAPI_KEY, "q": f"{lat},{lon}", "dt": target_date}
        headers = {"User-Agent": USER_AGENT}
        response = requests.get(url, params=params, headers=headers, timeout=10)

        if response.status_code != 200:
            return None

        day_data = response.json()["forecast"]["forecastday"][0]["day"]
        avg_temp = day_data.get("avgtemp_c")
        if avg_temp is None:
            return None

        return {
            "date": target_date,
            "avg_temp_c": round(avg_temp, 1),
            "max_temp_c": round(day_data.get("maxtemp_c", 0), 1),
            "min_temp_c": round(day_data.get("mintemp_c", 0), 1),
            "condition": day_data.get("condition", {}).get("text", "Unknown"),
        }
    except Exception as e:
        print(f"âŒ å¤©æ°”æ•°æ®è·å–å¤±è´¥ï¼š{str(e)}")
        return None


# ===================== 4. ä¸»æµç¨‹ï¼šå¯¼å‡º =====================
def export_city_data(city_name, country_code):
    print(f"\n{'=' * 60}\nğŸŒ æ­£åœ¨å¤„ç†ï¼š{city_name} ({country_code})")

    osm_data = get_osm_data(city_name, country_code)
    if not osm_data:
        return False

    wiki_content, infobox_cleaned, lang, wiki_title, wiki_url = get_wikipedia_data(
        city_name, country_code
    )
    if not wiki_content:
        return False

    chunks = split_text(wiki_content)
    print(f"âœ… æ–‡æœ¬åˆ†å—å®Œæˆï¼š{len(chunks)}æ®µ")
    chunks_with_vectors = vectorize_chunks(chunks)
    weather_data = get_weather_data(osm_data["lat"], osm_data["lon"])

    safe_city_name = city_name.replace(" ", "_")
    filename = f"{OUTPUT_DIR}/{safe_city_name}_{country_code}_{lang}.json"

    data = {
        "city_name": city_name,
        "country_code": country_code,
        "lang": lang,
        "timestamp": datetime.now().isoformat(),
        "urls": {
            "wikipedia": wiki_url,
            "weatherapi_query": f"http://api.weatherapi.com/v1/history.json?q={osm_data['lat']},{osm_data['lon']}&dt={weather_data['date']}"
            if weather_data
            else None,
            "osm_location": f"https://www.openstreetmap.org/search?query={city_name}%20{country_code}",
        },
        "location": {
            "latitude": osm_data["lat"],
            "longitude": osm_data["lon"],
            "country": osm_data["country"],
        },
        "knowledge": {
            "infobox": infobox_cleaned,
            "text_chunks": [
                {"text": text, "embedding": vec} for text, vec in chunks_with_vectors
            ],
        },
        "weather": weather_data,
    }

    os.makedirs(OUTPUT_DIR, exist_ok=True)
    try:
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“ âœ… æ•°æ®å·²å¯¼å‡ºï¼š{filename}")
        return True
    except Exception as e:
        print(f"âŒ æ–‡ä»¶å†™å…¥å¤±è´¥ï¼š{str(e)}")
        return False


# ===================== ä¸»ç¨‹åºå…¥å£ =====================
if __name__ == "__main__":
    print("â³ æ­£åœ¨åŠ è½½ BGE-M3 æ¨¡å‹ï¼ˆé¦–æ¬¡è¿è¡Œéœ€ä¸‹è½½ï¼Œçº¦1.5GBï¼‰...")
    print(f"ğŸ“Œ æ¨¡å‹ï¼š{MODEL_NAME}")

    os.makedirs(OUTPUT_DIR, exist_ok=True)

    cities = [
        ("Barcelona", "es"),
        ("Sanya", "cn"),
        ("Paris", "fr"),
        ("Athens", "gr"),
        ("Kyoto", "jp"),
        ("Beijing", "cn"),
    ]

    success_count = 0
    for name, code in cities:
        if export_city_data(name, code):
            success_count += 1

    print(f"\n{'=' * 60}")
    print(f"ğŸ‰ æ•°æ®å¯¼å‡ºå®Œæˆï¼æˆåŠŸ {success_count}/{len(cities)} ä¸ªåŸå¸‚")
    print(f"ğŸ“‚ æ‰€æœ‰æ–‡ä»¶å·²ä¿å­˜è‡³ï¼š{OUTPUT_DIR}")
