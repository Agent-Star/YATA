# NLU æœåŠ¡æ€§èƒ½ä¼˜åŒ–æ–¹æ¡ˆ

## æ¦‚è¿°

æœ¬æ–¹æ¡ˆåŸºäº [RAGè¿”å›åNLUè¶…æ—¶åˆ†æ](./RAGè¿”å›åNLUè¶…æ—¶åˆ†æ.md) ä¸­çš„å‘ç°ï¼Œé‡‡çº³ä»¥ä¸‹ä¸¤ä¸ªæ ¸å¿ƒè§£å†³æ–¹æ¡ˆï¼š

1. **ä¸­æœŸæ–¹æ¡ˆ**: å¹¶å‘æ‰§è¡Œç‹¬ç«‹çš„ LLM è°ƒç”¨ï¼ˆç«‹å³å¯å®æ–½ï¼‰
2. **é•¿æœŸæ–¹æ¡ˆ**: æµå¼å“åº”æ¶æ„ï¼ˆå‚è€ƒ backend/planner_routes.py çš„ SSE å®ç°ï¼‰

## æ–¹æ¡ˆç›®æ ‡

### æ€§èƒ½æŒ‡æ ‡

**å½“å‰çŠ¶æ€**:

- æ­£å¸¸æµç¨‹: 17-22 ç§’
- ä¸€æ¬¡ Verifier é‡è¯•: 44 ç§’ âŒ è¶…æ—¶

**ä¼˜åŒ–ç›®æ ‡**:

- é˜¶æ®µ 1 (å¹¶å‘è°ƒç”¨): 11-16 ç§’ âœ… èŠ‚çœ 6 ç§’
- é˜¶æ®µ 2 (æµå¼å“åº”): ç”¨æˆ·ä½“éªŒæå‡ï¼Œé¿å…é•¿æ—¶é—´ç­‰å¾…

### æ¶æ„ç›®æ ‡

- ä¿æŒç°æœ‰ async åŸºç¡€è®¾æ–½
- å‘åå…¼å®¹ç°æœ‰ API
- æ¸è¿›å¼è¿ç§»ï¼Œæ— éœ€åœæœº
- ä¸ºæœªæ¥æ‰©å±•ç•™å‡ºç©ºé—´

---

## é˜¶æ®µ 1: å¹¶å‘æ‰§è¡Œç‹¬ç«‹çš„ LLM è°ƒç”¨

### 1.1 é—®é¢˜åˆ†æ

**å½“å‰ä¸²è¡Œæ‰§è¡Œ** (`adviser_main.py:183-189`):

```python
# å½“å‰å®ç° - ä¸²è¡Œç­‰å¾…
result["context_summary"] = await run_context_summary(...)  # ç­‰å¾… 1-2s
result["plan_steps"] = await run_plan_actions(...)          # ç­‰å¾… 1-2s
result["final_aggregation"] = await run_aggregate(...)      # ç­‰å¾… 1-2s
result["detailed_itinerary"] = await generate_itinerary(...)# ç­‰å¾… 5-10s

# æ€»è®¡: 8-16 ç§’
```

**å¯å¹¶å‘çš„è°ƒç”¨**:

ä»¥ä¸‹ 3 ä¸ª LLM è°ƒç”¨**ç›¸äº’ç‹¬ç«‹**ï¼Œå¯ä»¥å¹¶å‘æ‰§è¡Œï¼š

- `run_context_summary()`
- `run_plan_actions()`
- `run_aggregate()`

**ä¾èµ–å…³ç³»**:

- `generate_itinerary()` ä¾èµ–äºä¸Šè¿° 3 ä¸ªè°ƒç”¨çš„ç»“æœï¼Œå¿…é¡»ç­‰å¾…å®ƒä»¬å®Œæˆ

### 1.2 å®æ–½æ–¹æ¡ˆ

#### ä¿®æ”¹æ–‡ä»¶: `NLU_module/agents/adviser/adviser_main.py`

**åŸä»£ç ** (çº¦ 183-189 è¡Œ):

```python
# ä¸²è¡Œæ‰§è¡Œ
result["context_summary"] = await run_context_summary(
    self.llm, user_input, doc_summaries
)
result["plan_steps"] = await run_plan_actions(
    self.llm, result["intent_parsed"]
)
result["final_aggregation"] = await run_aggregate(
    self.llm, [], result["intent_parsed"]
)
```

**ä¼˜åŒ–åä»£ç **:

```python
# å¹¶å‘æ‰§è¡Œç‹¬ç«‹çš„ LLM è°ƒç”¨
context_task = run_context_summary(self.llm, user_input, doc_summaries)
plan_task = run_plan_actions(self.llm, result["intent_parsed"])
aggregate_task = run_aggregate(self.llm, [], result["intent_parsed"])

# ä½¿ç”¨ asyncio.gather å¹¶å‘ç­‰å¾…
context_summary, plan_steps, final_aggregation = await asyncio.gather(
    context_task,
    plan_task,
    aggregate_task,
)

# å°†ç»“æœèµ‹å€¼åˆ° result å­—å…¸
result["context_summary"] = context_summary
result["plan_steps"] = plan_steps
result["final_aggregation"] = final_aggregation
```

**éœ€è¦å¯¼å…¥**:

```python
import asyncio  # åœ¨æ–‡ä»¶é¡¶éƒ¨æ·»åŠ 
```

#### é¢„æœŸæ•ˆæœ

**ä¸²è¡Œæ‰§è¡Œæ—¶é—´**:

- context_summary: 1-2s
- plan_steps: 1-2s
- final_aggregation: 1-2s
- **æ€»è®¡: 3-6 ç§’**

**å¹¶å‘æ‰§è¡Œæ—¶é—´**:

- 3 ä¸ªè°ƒç”¨å¹¶å‘æ‰§è¡Œ
- **æ€»è®¡: max(1-2s, 1-2s, 1-2s) = 1-2 ç§’**

**èŠ‚çœæ—¶é—´**: **4-6 ç§’** âœ…

#### é”™è¯¯å¤„ç†

ä½¿ç”¨ `asyncio.gather` çš„ `return_exceptions=True` å‚æ•°å¤„ç†éƒ¨åˆ†å¤±è´¥ï¼š

```python
results = await asyncio.gather(
    context_task,
    plan_task,
    aggregate_task,
    return_exceptions=True,  # ä¸ä¼šå› ä¸ºå•ä¸ªä»»åŠ¡å¤±è´¥è€Œå…¨éƒ¨å¤±è´¥
)

# æ£€æŸ¥æ¯ä¸ªç»“æœ
context_summary, plan_steps, final_aggregation = results

if isinstance(context_summary, Exception):
    logger.error(f"context_summary å¤±è´¥: {context_summary}")
    context_summary = ""  # ä½¿ç”¨é»˜è®¤å€¼

if isinstance(plan_steps, Exception):
    logger.error(f"plan_steps å¤±è´¥: {plan_steps}")
    plan_steps = []

if isinstance(final_aggregation, Exception):
    logger.error(f"final_aggregation å¤±è´¥: {final_aggregation}")
    final_aggregation = ""

result["context_summary"] = context_summary
result["plan_steps"] = plan_steps
result["final_aggregation"] = final_aggregation
```

### 1.3 å…¶ä»–æ½œåœ¨å¹¶å‘ä¼˜åŒ–ç‚¹

#### Intent Parsing ä¸­çš„å¹¶å‘

**å½“å‰ä¸²è¡Œæ‰§è¡Œ** (`adviser_intent.py:44-59`):

```python
# æ„å›¾è¯†åˆ«
intent_parsed = await self.llm.ask_json(prompt_parse_intent, ...)

# æ—¥æœŸè§„èŒƒåŒ– (æ¡ä»¶æ‰§è¡Œ)
if needs_date_normalization:
    normalized_dates = await self.llm.ask_json(prompt_normalize_date, ...)

# æŸ¥è¯¢æ”¹å†™
rewritten_query = await self.llm.ask_text(prompt_query_rewrite, ...)
```

**æ½œåœ¨ä¼˜åŒ–** (è°¨æ…ä½¿ç”¨):

æ—¥æœŸè§„èŒƒåŒ–å’ŒæŸ¥è¯¢æ”¹å†™**å¯èƒ½ç‹¬ç«‹**ï¼Œå¯å°è¯•å¹¶å‘ï¼š

```python
# å…ˆæ‰§è¡Œæ„å›¾è¯†åˆ«
intent_parsed = await self.llm.ask_json(prompt_parse_intent, ...)

# å¹¶å‘æ‰§è¡Œæ—¥æœŸè§„èŒƒåŒ–å’ŒæŸ¥è¯¢æ”¹å†™
tasks = []
if needs_date_normalization:
    tasks.append(self.llm.ask_json(prompt_normalize_date, ...))
else:
    tasks.append(None)  # å ä½

tasks.append(self.llm.ask_text(prompt_query_rewrite, ...))

results = await asyncio.gather(*tasks, return_exceptions=True)
normalized_dates = results[0] if results[0] is not None else None
rewritten_query = results[1]
```

**æ³¨æ„**: éœ€è¦ä»”ç»†éªŒè¯æŸ¥è¯¢æ”¹å†™æ˜¯å¦ä¾èµ–äºæ—¥æœŸè§„èŒƒåŒ–çš„ç»“æœã€‚

---

## é˜¶æ®µ 2: æµå¼å“åº”æ¶æ„

### 2.1 è®¾è®¡ç›®æ ‡

- ç”¨æˆ·è¾¹ç­‰å¾…è¾¹çœ‹åˆ°éƒ¨åˆ†ç»“æœï¼Œé¿å…"é»‘å±ç­‰å¾…"
- å³ä½¿æ€»æ—¶é—´ä¸å˜ï¼Œç”¨æˆ·ä½“éªŒå¤§å¹…æå‡
- å…¼å®¹ backend çš„ SSE (Server-Sent Events) å®ç°

### 2.2 Backend æµå¼å®ç°åˆ†æ

**å‚è€ƒ**: `backend/src/service/planner_routes.py` (dev åˆ†æ”¯)

**æ ¸å¿ƒæ¨¡å¼**:

```python
@planner_router.post("/plan/stream")
async def plan_stream(...) -> StreamingResponse:
    async def generate_events() -> AsyncGenerator[str, None]:
        try:
            # è·å– agent å’Œ config
            agent: AgentGraph = get_agent(DEFAULT_AGENT)
            config = RunnableConfig(configurable={"thread_id": thread_id, ...})

            # æµå¼è°ƒç”¨ agent.astream()
            async for stream_event in agent.astream(
                user_input,
                config=config,
                stream_mode=["messages"],  # é€ token æµå¼
                subgraphs=True
            ):
                if stream_mode == "messages":
                    msg, _ = event
                    if isinstance(msg, AIMessageChunk):
                        content = remove_tool_calls(msg.content)
                        if content:
                            # å‘é€ SSE äº‹ä»¶
                            yield f"data: {json.dumps({'type': 'token', 'delta': convert_message_content_to_string(content)})}\n\n"

            # å‘é€ç»“æŸäº‹ä»¶
            yield f"data: {json.dumps({'type': 'end', 'messageId': message_id, 'metadata': {}})}\n\n"
            yield "data: [DONE]\n\n"

        except Exception as e:
            logger.error(f"æµå¼è§„åˆ’å¤±è´¥: {e}")
            yield f"data: {json.dumps({'type': 'error', 'content': 'æœåŠ¡å™¨å¼‚å¸¸'})}\n\n"

    return StreamingResponse(
        generate_events(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",  # ç¦ç”¨ Nginx ç¼“å†²
        },
    )
```

**å…³é”®æŠ€æœ¯**:

1. **FastAPI StreamingResponse** - è¿”å›å¼‚æ­¥ç”Ÿæˆå™¨
2. **SSE æ ¼å¼** - `data: {JSON}\n\n`
3. **AIMessageChunk** - LangChain çš„æµå¼æ¶ˆæ¯ç±»å‹
4. **stream_mode=["messages"]** - é€ token æµå¼è¾“å‡º

### 2.3 NLU æµå¼å®ç°æ–¹æ¡ˆ

#### æ–¹æ¡ˆ A: æ¸è¿›å¼æµå¼è¾“å‡ºï¼ˆæ¨èï¼‰

**ä¼˜ç‚¹**: åˆ†é˜¶æ®µæµå¼ï¼Œå®ç°ç®€å•ï¼Œå‘åå…¼å®¹

**å®ç°**: åœ¨æ¯ä¸ªé˜¶æ®µå®Œæˆæ—¶å‘é€äº‹ä»¶

```python
# fastapi_server.py - æ–°å¢æµå¼ç«¯ç‚¹

@app.post("/nlu/simple/stream")
async def nlu_simple_stream(request: NLURequest):
    """
    æµå¼ NLU æ¥å£ - æ¸è¿›å¼è¿”å›ç»“æœ

    äº‹ä»¶ç±»å‹:
    - phase_start: é˜¶æ®µå¼€å§‹ {"type": "phase_start", "phase": "intent_parsing"}
    - phase_end: é˜¶æ®µå®Œæˆ {"type": "phase_end", "phase": "intent_parsing", "result": {...}}
    - token: è¡Œç¨‹ç”Ÿæˆçš„ token {"type": "token", "delta": "..."}
    - end: å¤„ç†å®Œæˆ {"type": "end", "session_id": "..."}
    - error: é”™è¯¯ {"type": "error", "message": "..."}
    """
    async def generate_events():
        session_id = request.session_id or str(uuid4())

        try:
            # è·å–æˆ–åˆ›å»ºä¼šè¯
            if session_id not in SESSIONS:
                SESSIONS[session_id] = NLU(
                    log_folder="log",
                    file_name=session_id,
                    with_verifier=True
                )
                SESSIONS.move_to_end(session_id)
                logger.info(f"åˆ›å»ºæ–°ä¼šè¯: {session_id}")

            nlu = SESSIONS[session_id]
            SESSIONS.move_to_end(session_id)

            # === é˜¶æ®µ 1: Intent Parsing ===
            yield sse_event({"type": "phase_start", "phase": "intent_parsing"})

            intent_result = await nlu.adviser.run_intent_parsing(request.text)

            yield sse_event({
                "type": "phase_end",
                "phase": "intent_parsing",
                "result": intent_result
            })

            # === é˜¶æ®µ 2: RAG æ£€ç´¢ ===
            yield sse_event({"type": "phase_start", "phase": "rag_search"})

            rag_results = await nlu.adviser.call_rag_api(...)

            yield sse_event({
                "type": "phase_end",
                "phase": "rag_search",
                "result": {"count": len(rag_results)}
            })

            # === é˜¶æ®µ 3: å†…å®¹ç”Ÿæˆ (å¹¶å‘) ===
            yield sse_event({"type": "phase_start", "phase": "content_generation"})

            # å¹¶å‘æ‰§è¡Œç‹¬ç«‹è°ƒç”¨
            context_task = nlu.adviser.run_context_summary(...)
            plan_task = nlu.adviser.run_plan_actions(...)
            aggregate_task = nlu.adviser.run_aggregate(...)

            context, plan, aggregate = await asyncio.gather(
                context_task, plan_task, aggregate_task
            )

            yield sse_event({
                "type": "phase_end",
                "phase": "content_generation"
            })

            # === é˜¶æ®µ 4: è¡Œç¨‹ç”Ÿæˆ (æµå¼) ===
            yield sse_event({"type": "phase_start", "phase": "itinerary_generation"})

            # è¿™é‡Œéœ€è¦ä¿®æ”¹ generate_itinerary æ”¯æŒæµå¼è¾“å‡º (è§ä¸‹æ–‡)
            async for token in nlu.adviser.generate_itinerary_stream(...):
                yield sse_event({"type": "token", "delta": token})

            yield sse_event({"type": "phase_end", "phase": "itinerary_generation"})

            # === é˜¶æ®µ 5: Verifier å®¡æŸ¥ ===
            if nlu.with_verifier:
                yield sse_event({"type": "phase_start", "phase": "verification"})

                # Verifier æ£€æŸ¥
                # ...

                yield sse_event({"type": "phase_end", "phase": "verification"})

            # === å®Œæˆ ===
            yield sse_event({
                "type": "end",
                "session_id": session_id,
                "status": "complete"
            })
            yield "data: [DONE]\n\n"

        except Exception as e:
            logger.error(f"æµå¼å¤„ç†å¤±è´¥: {e}")
            yield sse_event({"type": "error", "message": str(e)})
            yield "data: [DONE]\n\n"

    return StreamingResponse(
        generate_events(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
        },
    )


def sse_event(data: dict) -> str:
    """ç”Ÿæˆ SSE äº‹ä»¶å­—ç¬¦ä¸²"""
    return f"data: {json.dumps(data, ensure_ascii=False)}\n\n"
```

#### æ–¹æ¡ˆ B: å®Œå…¨æµå¼è¾“å‡ºï¼ˆé•¿æœŸï¼‰

**ä¼˜ç‚¹**: æœ€ä½³ç”¨æˆ·ä½“éªŒï¼Œé€ token æµå¼

**å®ç°**: ä¿®æ”¹ `generate_itinerary` æ”¯æŒæµå¼è¾“å‡º

```python
# adviser_itinerary.py - ä¿®æ”¹ generate_itinerary

async def generate_itinerary_stream(
    self,
    llm,
    intent_parsed,
    context_summary,
    plan_steps,
    final_aggregation
) -> AsyncGenerator[str, None]:
    """
    æµå¼ç”Ÿæˆè¡Œç¨‹è§„åˆ’

    Yields:
        str: æ¯æ¬¡ç”Ÿæˆçš„ token
    """
    # æ„å»º prompt
    itinerary_prompt = build_itinerary_prompt(
        intent_parsed,
        context_summary,
        plan_steps,
        final_aggregation
    )

    # ä½¿ç”¨ LLM çš„æµå¼ API
    async for chunk in llm.ask_text_stream(
        itinerary_prompt,
        temperature=0.6,
        max_tokens=12000
    ):
        yield chunk
```

**éœ€è¦ä¿®æ”¹ `model_definition.py`**:

```python
# source/model_definition.py

class LLMWrapper:
    async def ask_text_stream(
        self,
        prompt: str,
        temperature: float = 0.7,
        max_tokens: int = 2000
    ) -> AsyncGenerator[str, None]:
        """
        æµå¼è°ƒç”¨ LLMï¼Œé€ token è¿”å›

        Yields:
            str: æ¯æ¬¡ç”Ÿæˆçš„æ–‡æœ¬ chunk
        """
        messages = [{"role": "user", "content": prompt}]

        try:
            response = await self.client.chat.completions.create(
                model=self.model_name,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                stream=True,  # å¯ç”¨æµå¼è¾“å‡º
            )

            async for chunk in response:
                if chunk.choices and chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content

        except Exception as e:
            logger.error(f"æµå¼ LLM è°ƒç”¨å¤±è´¥: {e}")
            raise
```

### 2.4 å‰åç«¯é›†æˆ

**å‰ç«¯è°ƒç”¨ç¤ºä¾‹** (JavaScript):

```javascript
async function streamNLU(text, sessionId) {
    const response = await fetch('http://localhost:8010/nlu/simple/stream', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ text, session_id: sessionId })
    });

    const reader = response.body.getReader();
    const decoder = new TextDecoder();

    while (true) {
        const { done, value } = await reader.read();
        if (done) break;

        const chunk = decoder.decode(value);
        const lines = chunk.split('\n\n');

        for (const line of lines) {
            if (line.startsWith('data: ')) {
                const data = line.slice(6);
                if (data === '[DONE]') return;

                const event = JSON.parse(data);

                switch (event.type) {
                    case 'phase_start':
                        console.log(`å¼€å§‹: ${event.phase}`);
                        break;
                    case 'token':
                        // é€ token æ˜¾ç¤ºè¡Œç¨‹å†…å®¹
                        document.getElementById('itinerary').innerText += event.delta;
                        break;
                    case 'end':
                        console.log('å®Œæˆ');
                        break;
                    case 'error':
                        console.error('é”™è¯¯:', event.message);
                        break;
                }
            }
        }
    }
}
```

---

## é˜¶æ®µ 3: å…¶ä»–ä¼˜åŒ–å»ºè®®

### 3.1 å‡å°‘ generate_itinerary çš„ max_tokens

**å½“å‰**: `max_tokens=12000` (è€—æ—¶ 5-10 ç§’)

**ä¼˜åŒ–**: æ ¹æ®è¡Œç¨‹å¤©æ•°åŠ¨æ€è°ƒæ•´

```python
# adviser_itinerary.py

def calculate_max_tokens(duration: int) -> int:
    """
    æ ¹æ®è¡Œç¨‹å¤©æ•°è®¡ç®—åˆç†çš„ max_tokens

    ä¼°ç®—: æ¯å¤©éœ€è¦ 800-1000 tokens (çº¦ 120-150 å­—ä¸­æ–‡)
    """
    base_tokens = 2000  # åŸºç¡€éƒ¨åˆ† (æ ‡é¢˜ã€ä»‹ç»ã€æ€»ç»“)
    tokens_per_day = 1000

    max_tokens = base_tokens + (duration * tokens_per_day)

    # é™åˆ¶åœ¨åˆç†èŒƒå›´å†…
    return min(max(max_tokens, 4000), 12000)


# åœ¨ generate_itinerary ä¸­ä½¿ç”¨
max_tokens = calculate_max_tokens(intent_parsed.get("duration", 3))
markdown = await adviser.ask_text(
    itinerary_prompt,
    temperature=0.6,
    max_tokens=max_tokens  # åŠ¨æ€è°ƒæ•´
)
```

**é¢„æœŸæ•ˆæœ**:

- 3 å¤©è¡Œç¨‹: 5000 tokens â†’ è€—æ—¶ 3-5 ç§’ (èŠ‚çœ 2-5 ç§’)
- 7 å¤©è¡Œç¨‹: 9000 tokens â†’ è€—æ—¶ 4-8 ç§’ (èŠ‚çœ 1-2 ç§’)

### 3.2 è°ƒæ•´ Verifier é‡è¯•ç­–ç•¥

**å½“å‰**: `max_retries=3` (æœ€åæƒ…å†µ 88 ç§’)

**ä¼˜åŒ– 1: å‡å°‘é‡è¯•æ¬¡æ•°**

```python
# fastapi_server.py
SESSIONS[sid] = NLU(
    log_folder="log",
    file_name=sid,
    with_verifier=True,
    max_retries=1  # ä» 3 é™ä½åˆ° 1
)
```

**ä¼˜åŒ– 2: å¢é‡é‡è¯•**

ä¸é‡æ–°ç”Ÿæˆæ•´ä¸ªè¡Œç¨‹ï¼Œåªä¿®å¤æ£€æµ‹åˆ°çš„é—®é¢˜ï¼š

```python
# NLU_module/main.py

async def incremental_fix(response, issue):
    """
    å¢é‡ä¿®å¤ Verifier æ£€æµ‹åˆ°çš„é—®é¢˜

    è€Œä¸æ˜¯å®Œæ•´é‡æ–°ç”Ÿæˆ
    """
    # åªè°ƒç”¨ LLM ä¿®å¤ç‰¹å®šé—®é¢˜
    fix_prompt = f"""
    æ£€æµ‹åˆ°è¡Œç¨‹ä¸­çš„é—®é¢˜: {issue}

    è¯·ä¿®å¤ä»¥ä¸‹è¡Œç¨‹ä¸­çš„é—®é¢˜ (åªè¾“å‡ºä¿®å¤åçš„éƒ¨åˆ†):
    {response}
    """

    fixed_part = await self.adviser.llm.ask_text(fix_prompt, max_tokens=2000)

    # åˆå¹¶ä¿®å¤
    return merge_fix(response, fixed_part)
```

### 3.3 å¢åŠ è¶…æ—¶æ—¶é—´ (Quick Fix)

**å½“å‰**: `REQUEST_TIMEOUT = 28.0`

**ä¼˜åŒ–**: è€ƒè™‘ Verifier é‡è¯•çš„æƒ…å†µ

```python
# fastapi_server.py

# æ–¹æ¡ˆ 1: å›ºå®šå¢åŠ åˆ° 60s
REQUEST_TIMEOUT = 60.0

# æ–¹æ¡ˆ 2: æ ¹æ®æ˜¯å¦å¯ç”¨ Verifier åŠ¨æ€è°ƒæ•´
REQUEST_TIMEOUT = 60.0 if WITH_VERIFIER else 30.0
```

**æ³¨æ„**: éœ€è¦åŒæ­¥ä¿®æ”¹ backend çš„è¶…æ—¶è®¾ç½®

---

## å®æ–½è®¡åˆ’

### Phase 1: å¹¶å‘è°ƒç”¨ä¼˜åŒ– (ç«‹å³å®æ–½)

**æ—¶é—´**: 1-2 å¤©

**ä»»åŠ¡**:

1. âœ… ä¿®æ”¹ `adviser_main.py` - å¹¶å‘æ‰§è¡Œ 3 ä¸ªç‹¬ç«‹è°ƒç”¨
2. âœ… æ·»åŠ é”™è¯¯å¤„ç†å’Œæ—¥å¿—
3. âœ… æœ¬åœ°æµ‹è¯•éªŒè¯æ€§èƒ½æå‡
4. âœ… éƒ¨ç½²åˆ°å¼€å‘ç¯å¢ƒ

**é¢„æœŸæ•ˆæœ**: èŠ‚çœ 4-6 ç§’

### Phase 2: æ¸è¿›å¼æµå¼è¾“å‡º (1 å‘¨)

**æ—¶é—´**: 1 å‘¨

**ä»»åŠ¡**:

1. âœ… å®ç° `/nlu/simple/stream` ç«¯ç‚¹ (æ–¹æ¡ˆ A)
2. âœ… åˆ†é˜¶æ®µå‘é€ SSE äº‹ä»¶
3. âœ… å‰ç«¯é€‚é…æµå¼æ¥å£
4. âœ… æµ‹è¯•å’Œä¼˜åŒ–

**é¢„æœŸæ•ˆæœ**: ç”¨æˆ·ä½“éªŒå¤§å¹…æå‡

### Phase 3: å®Œå…¨æµå¼è¾“å‡º (2-3 å‘¨)

**æ—¶é—´**: 2-3 å‘¨

**ä»»åŠ¡**:

1. âœ… å®ç° `LLMWrapper.ask_text_stream()`
2. âœ… ä¿®æ”¹ `generate_itinerary` æ”¯æŒæµå¼
3. âœ… å®Œæ•´çš„é€ token æµå¼è¾“å‡º
4. âœ… æ€§èƒ½æµ‹è¯•å’Œè°ƒä¼˜

**é¢„æœŸæ•ˆæœ**: æœ€ä½³ç”¨æˆ·ä½“éªŒ

### Phase 4: å…¶ä»–ä¼˜åŒ– (æŒç»­)

**ä»»åŠ¡**:

1. âœ… åŠ¨æ€è°ƒæ•´ max_tokens
2. âœ… ä¼˜åŒ– Verifier é‡è¯•ç­–ç•¥
3. âœ… è°ƒæ•´è¶…æ—¶è®¾ç½®
4. âœ… ç›‘æ§å’Œæ—¥å¿—ä¼˜åŒ–

---

## æµ‹è¯•ç­–ç•¥

### å•å…ƒæµ‹è¯•

```python
# tests/test_concurrent_calls.py

import asyncio
import pytest
from NLU_module.agents.adviser.adviser_main import Adviser

@pytest.mark.asyncio
async def test_concurrent_generation():
    """æµ‹è¯•å¹¶å‘è°ƒç”¨æ˜¯å¦æ­£å¸¸å·¥ä½œ"""
    adviser = Adviser(llm=...)

    intent_parsed = {...}
    user_input = "..."
    doc_summaries = [...]

    # å¹¶å‘æ‰§è¡Œ
    context_task = adviser.run_context_summary(adviser.llm, user_input, doc_summaries)
    plan_task = adviser.run_plan_actions(adviser.llm, intent_parsed)
    aggregate_task = adviser.run_aggregate(adviser.llm, [], intent_parsed)

    results = await asyncio.gather(
        context_task,
        plan_task,
        aggregate_task,
        return_exceptions=True,
    )

    # éªŒè¯ç»“æœ
    assert all(not isinstance(r, Exception) for r in results)
    assert len(results) == 3


@pytest.mark.asyncio
async def test_streaming_output():
    """æµ‹è¯•æµå¼è¾“å‡º"""
    adviser = Adviser(llm=...)

    tokens = []
    async for token in adviser.generate_itinerary_stream(...):
        tokens.append(token)

    # éªŒè¯æµå¼è¾“å‡º
    assert len(tokens) > 0
    full_text = ''.join(tokens)
    assert len(full_text) > 100
```

### æ€§èƒ½æµ‹è¯•

```python
# tests/test_performance.py

import time
import pytest

@pytest.mark.asyncio
async def test_performance_improvement():
    """éªŒè¯å¹¶å‘è°ƒç”¨çš„æ€§èƒ½æå‡"""

    # ä¸²è¡Œæ‰§è¡Œ
    start = time.time()
    await sequential_execution()
    sequential_time = time.time() - start

    # å¹¶å‘æ‰§è¡Œ
    start = time.time()
    await concurrent_execution()
    concurrent_time = time.time() - start

    # éªŒè¯æ€§èƒ½æå‡
    improvement = sequential_time - concurrent_time
    assert improvement > 3, f"Expected >3s improvement, got {improvement:.2f}s"

    print(f"æ€§èƒ½æå‡: {improvement:.2f} ç§’")
```

### é›†æˆæµ‹è¯•

```bash
# æµ‹è¯•æµå¼æ¥å£
curl -N -X POST "http://localhost:8010/nlu/simple/stream" \
     -H "Content-Type: application/json" \
     -d '{
       "text": "è§„åˆ’ä¸€ä¸ª4å¤©çš„å·´é»è¡Œç¨‹ï¼Œé¢„ç®—8000å…ƒ"
     }'
```

---

## ç›‘æ§å’Œæ—¥å¿—

### æ€§èƒ½ç›‘æ§

```python
# åœ¨å…³é”®è·¯å¾„æ·»åŠ æ€§èƒ½ç›‘æ§

import time
from functools import wraps

def monitor_performance(phase_name: str):
    """æ€§èƒ½ç›‘æ§è£…é¥°å™¨"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            start = time.time()
            try:
                result = await func(*args, **kwargs)
                elapsed = time.time() - start
                logger.info(f"[PERF] {phase_name}: {elapsed:.2f}s")
                return result
            except Exception as e:
                elapsed = time.time() - start
                logger.error(f"[PERF] {phase_name} FAILED after {elapsed:.2f}s: {e}")
                raise
        return wrapper
    return decorator


# ä½¿ç”¨ç¤ºä¾‹
@monitor_performance("context_summary")
async def run_context_summary(llm, user_input, doc_summaries):
    # ...
```

### æ—¥å¿—å¢å¼º

```python
# åœ¨ fastapi_server.py æ·»åŠ è¯·æ±‚çº§åˆ«çš„æ—¥å¿—

@app.post("/nlu/simple/stream")
async def nlu_simple_stream(request: NLURequest):
    request_id = str(uuid4())[:8]
    logger.info(f"[{request_id}] å¼€å§‹å¤„ç†æµå¼è¯·æ±‚: {request.text[:50]}...")

    start_time = time.time()

    async def generate_events():
        try:
            # ... å¤„ç†é€»è¾‘ ...

            elapsed = time.time() - start_time
            logger.info(f"[{request_id}] å®Œæˆå¤„ç†ï¼Œæ€»è€—æ—¶: {elapsed:.2f}s")

        except Exception as e:
            elapsed = time.time() - start_time
            logger.error(f"[{request_id}] å¤„ç†å¤±è´¥ï¼Œè€—æ—¶: {elapsed:.2f}s, é”™è¯¯: {e}")

    return StreamingResponse(generate_events(), ...)
```

---

## é£é™©å’Œç¼“è§£

### é£é™© 1: å¹¶å‘è°ƒç”¨å¯¼è‡´ API é™æµ

**é£é™©**: å¹¶å‘è¯·æ±‚å¯èƒ½è§¦å‘ Azure OpenAI çš„é€Ÿç‡é™åˆ¶

**ç¼“è§£**:

- ç›‘æ§ 429 é”™è¯¯
- å®ç°æŒ‡æ•°é€€é¿é‡è¯•
- è°ƒæ•´å¹¶å‘æ•°é‡

```python
# æ·»åŠ é‡è¯•é€»è¾‘
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=10)
)
async def call_llm_with_retry(llm, prompt, **kwargs):
    return await llm.ask_text(prompt, **kwargs)
```

### é£é™© 2: æµå¼è¾“å‡ºä¸­æ–­

**é£é™©**: å®¢æˆ·ç«¯æ–­å¼€è¿æ¥å¯¼è‡´æµå¼è¾“å‡ºä¸­æ–­

**ç¼“è§£**:

- æ•è· `asyncio.CancelledError`
- æ¸…ç†èµ„æº
- æ—¥å¿—è®°å½•

```python
async def generate_events():
    try:
        # ... æµå¼è¾“å‡º ...
    except asyncio.CancelledError:
        logger.warning(f"å®¢æˆ·ç«¯æ–­å¼€è¿æ¥ï¼Œåœæ­¢æµå¼è¾“å‡º")
        # æ¸…ç†èµ„æº
        raise
```

### é£é™© 3: å‘åå…¼å®¹æ€§

**é£é™©**: æ–°æ¥å£å¯èƒ½ç ´åç°æœ‰é›†æˆ

**ç¼“è§£**:

- ä¿ç•™åŸæœ‰ `/nlu/simple` æ¥å£
- æ–°å¢ `/nlu/simple/stream` æ¥å£
- å‰ç«¯æ¸è¿›å¼è¿ç§»

---

## æ€»ç»“

### é¢„æœŸæ€§èƒ½æå‡

| ä¼˜åŒ–é¡¹ | èŠ‚çœæ—¶é—´ | å®æ–½éš¾åº¦ | ä¼˜å…ˆçº§ |
|--------|----------|----------|--------|
| å¹¶å‘æ‰§è¡Œ 3 ä¸ª LLM è°ƒç”¨ | 4-6 ç§’ | ä½ | ğŸ”´ é«˜ |
| åŠ¨æ€è°ƒæ•´ max_tokens | 1-5 ç§’ | ä½ | ğŸŸ¡ ä¸­ |
| å‡å°‘ Verifier é‡è¯• | 20-40 ç§’ | ä½ | ğŸ”´ é«˜ |
| å¢åŠ è¶…æ—¶æ—¶é—´ | - | ä½ | ğŸ”´ é«˜ |
| æ¸è¿›å¼æµå¼è¾“å‡º | ç”¨æˆ·ä½“éªŒ | ä¸­ | ğŸŸ¡ ä¸­ |
| å®Œå…¨æµå¼è¾“å‡º | ç”¨æˆ·ä½“éªŒ | é«˜ | ğŸŸ¢ ä½ |

### æœ€ç»ˆæ•ˆæœ

**é˜¶æ®µ 1 å®Œæˆå** (å¹¶å‘è°ƒç”¨ + Quick Fix):

- æ­£å¸¸æµç¨‹: **11-16 ç§’** (åŸ 17-22 ç§’)
- ä¸€æ¬¡ Verifier é‡è¯•: **22-32 ç§’** (åŸ 44 ç§’) âœ… ä¸è¶…æ—¶
- è¶…æ—¶é™åˆ¶: **60 ç§’**

**é˜¶æ®µ 2 å®Œæˆå** (æµå¼è¾“å‡º):

- ç”¨æˆ·åœ¨ **2-3 ç§’å†…** çœ‹åˆ°ç¬¬ä¸€ä¸ªç»“æœ
- è¾¹ç­‰å¾…è¾¹çœ‹åˆ°è¡Œç¨‹é€æ­¥ç”Ÿæˆ
- æ€»ä½“ç”¨æˆ·ä½“éªŒæå‡ **80%+**

### ä¸‹ä¸€æ­¥è¡ŒåŠ¨

1. **ç«‹å³**: å®æ–½é˜¶æ®µ 1 (å¹¶å‘è°ƒç”¨ä¼˜åŒ–)
2. **æœ¬å‘¨**: å¢åŠ è¶…æ—¶æ—¶é—´ + å‡å°‘ Verifier é‡è¯•
3. **ä¸‹å‘¨**: å¯åŠ¨é˜¶æ®µ 2 (æ¸è¿›å¼æµå¼è¾“å‡º)
4. **æœ¬æœˆ**: å®Œæˆæ€§èƒ½æµ‹è¯•å’Œç›‘æ§
